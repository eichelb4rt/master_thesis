\chapter{Proofs}
\label{chap:proofs}

\section{Exponential Blow-Up}
% TODO: what do we prove?
% (Proof that expanding the matrix multiplication in a neural network results in an exponentially big term in terms of the width of the skipped layer of the neural network:)

Given: fully connected neural network with two layers ($n \rightarrow m \rightarrow l$ neurons), ReLU Activations, maps inputs $x \in \R^n$ to outputs $y \in R^l$, with parameters $A^{(0)} \in \R^{m \times n}, A^{(1)} \in \R^{l \times m}, b^{(0)} \in \R^m, b^{(1)} \in \R^l$.
Then the computation of the neural network is:
$$y = \max(A^{(1)}\max(A^{(0)}x + b^{(0)}, 0) + b^{(1)}, 0)$$

To reasonably work with matrix multiplication in the tropcial semiring, we can only view matrices with positive integer entries.
Making the entries integers does not impact the strength of the neural network, because [...] (quote the paper).

Now to only use positive valued matrices, we can rewrite the expression of computing the next layer from a previous layer:
\begin{align*}
    \max(Ax + b, 0) & = \max(A_+ x - A_- x + b, A_- x - A_- x) \\
                    & = \max(A_+ x + b, A_- x) - A_- x
\end{align*}
where $A_+ = \max(A, 0), A_- = \max(-A, 0)$ and therefore $A = A_+ - A_-$.

This turns the network output into a tropical rational function (quote):
\begin{align*}
    y & = \max(\overbrace{A^{(1)}_+ \max(A^{(0)}_+ x + b^{(0)}, A^{(0)}_- x)}^z + A^{(1)}_- A^{(0)}_+ x + b^{(1)}, \\
      & \phantom{{} =} A^{(1)}_- \max(A^{(0)}_+ x + b^{(0)}, A^{(0)}_- x) + A^{(1)}_+ A^{(0)}_+ x)                 \\
      & \phantom{{} =} -\left[A^{(1)}_- \max(A^{(0)}_+ x + b^{(0)}, A^{(0)}_- x) + A^{(1)}_+ A^{(0)}_+ x\right]
\end{align*}
We focus on the subexpression $z$, which makes the calculation a bit simpler, but keeps the point.

Now if we want to avoid switching semirings, we need to apply the distributive law a bunch of times.
\begin{align*}
    z   & = A^{(1)}_+ \max(A^{(0)}_+ x + b^{(0)}, A^{(0)}_- x)                                                                                                                                                                                                                                                                                                          \\
    z_i & = \bigodot\limits_{j = 1}^{m}\left(b^{(0)}_j \odot \bigodot\limits_{k = 1}^{n} x_k^{\odot A^{(0)}_{jk+}} \oplus \bigodot\limits_{k = 1}^{n} x_k^{\odot A^{(0)}_{jk-}}\right)^{\odot A^{(1)}_{ij+}}                                                                                                                                                            \\
        & = \bigodot\limits_{j = 1}^{m}\left(\left(b^{(0)}_j\right)^{\odot A^{(1)}_{ij+}} \odot \bigodot\limits_{k = 1}^{n} x_k^{\odot \left(A^{(1)}_{ij+} + A^{(0)}_{jk+}\right)} \oplus \bigodot\limits_{k = 1}^{n} x_k^{\odot \left(A^{(1)}_{ij+} + A^{(0)}_{jk-}\right)}\right)                                                                                     \\
        & = \bigoplus\limits_{J \in 2^{[m]}} \bigodot\limits_{j \in J} \left[\left(b^{(0)}_j\right)^{\odot A^{(1)}_{ij+}} \odot \bigodot\limits_{k = 1}^{n} x_k^{\odot \left(A^{(1)}_{ij+} + A^{(0)}_{jk+}\right)}\right] \odot \bigodot\limits_{j \in [n] \setminus J} \left[\bigodot\limits_{k = 1}^{n} x_k^{\odot \left(A^{(1)}_{ij+} + A^{(0)}_{jk-}\right)}\right]
\end{align*}
Where the second equality is just the first equality written with the operations of the tropical semiring,
the third equality follows from the distributive law with standard operations,
and the last equality follows from the distributive law in the tropical semiring.

This expression maximizes over a number of subexpressions that grows exponentially in the width of the inner layer.
Which subexpressions can be removed before the evaluation remains an open question.
Note that it depends on the non-linearities of the neural network, which might make it hard to find a general answer to this question.
