\section{Einsum}
Given two third-order tensor $A \in \R^{3 \times 4 \times 5}, B \in \R^{3 \times 3 \times 5}$ and a vector $v \in \R^4$.
Consider the following computation for a matrix $C \in \R^{3 \times 3}$:
$$\forall i \in [3]: \forall j \in [4]: C_{ij} = \sum\limits_{k = 1}^{5} A_{ijk} B_{iik} v_j.$$
This can be written in a much shorter form with einsum:
$$C = (ijk, iik, j \rightarrow ij, A, B, v)$$

Through the following definition, we hope to clear up why this einsum expression results in the computation above,
and what computation a general einsum expression results in.

\begin{definition}
    Einsum expressions generally evaluate to some computation over tensors, so let $T^{(1)},\dots,T^{(n)}$ be our input tensors,
    where $T^{(i)}$ is a $n_i$-th order tensor for $i \in [n]$.
    The core of the einsum expression are index strings. For this, we first need a collection of symbols $S$.
    The respective index string for a tensor $T^{(i)}$ is then just a tuple $\bm{s_i} \in S^{n_i}$,
    composed of symbols $s_{ij} \in S$ for $j \in [n_i]$.
    We refer to the index string that is right of the arrow ($\rightarrow$) as the output string $\bm{s_t}$.

    In our example this could be $S = \smallset{i,j,k}$ with respective index strings
    $\bm{s_1} = ijk$ for $T^{(1)} = A$,
    $\bm{s_2} = iik$ for $T^{(2)} = B$,
    $\bm{s_3} = j$ for $T^{(3)} = v$,
    and $\bm{s_t} = ij$.
    The individual symbols are $s_{11} = i$, $s_{12} = j$, $s_{13} = k$, $s_{12} = i$, $s_{22} = i$, $s_{23} = k$, $s_{31} = j$, $s_{t1} = i$, $s_{t2} = j$.

    In order to refer to individual tensor axes, let us numerate them with $a_{ij} \in \mathbb{N}$,
    where $a_{ij}$ denotes the $j$-th axis of the tensor $i$-th tensor $T^{(i)}$.
    The actual value of $a_{ij}$ does not matter.
    These variables are just used as unique identifiers for the axes.

    The next step in the definition is to speak about the axis sizes.
    If we want to iterate over shared indices, it would be nice if the axes, that these indices are used for, share the same size.
    In our example, $A_{ijk}$ and $v_j$ share the symbol $s_{12} = s_{31} = j$.
    This means that the respective axes $a_{12}$ and $a_{31}$ have to have the same size, which happens to be 4 (four?).
    Let us express this formally.

    Let $d_{ij}$ denote the size of the axis $a_{ij}$ for $i \in [n], j \in [n_i]$.
    Then it must hold that $s_{ij} = s_{i'j'} \implies d_{ij} = d_{i'j'}$ for all $i,i' \in [n], j \in [n_i], j' \in [n_{i'}]$.

    Therefore we can also denote the size of all axes that a symbol $s \in S$ corresponds to as $d_s := d_{ij}$ for all $i \in [n], j \in [n_i]$ with $s = s_{ij}$.
    Note that not all same size axes have to assigned the same symbol. E.g. a square matrix could have index strings $\bm{s} = (i, i)$ or $\bm{s} = (i, j)$.

    The next step of the definition is figuring out which symbols are used for summation and which symbols are used for saving the result of the computation.
    In order to do this, it is useful to know which symbols are in an index string, because symbols can occur more than once in just one index string (as seen in $B_{iik}$ in our example).
    Therefore, let $\sigma(\bm{s})$ denote the set with all symbols used in an index string $\bm{s}$.
    E.g. $\sigma(\bm{s_2}) = \sigma(iik) = \smallset{ik}$.

    All symbols to the right of the arrow ($\rightarrow$) are used as an index for the result of the computation.
    These symbols are called \textit{bound} symbols $B = \sigma(\bm{s_t})$.
    All other symbols used in the expression are called \textit{free} symbols $F = \bigcup_{i \in [n]} \sigma(\bm{s_i}) \setminus \sigma(\bm{s_t})$.
    In einsum, we sum over all axes that belong to free symbols.
    It follows that the multi-index space we iterate over is $\mathcal{I} = \prod_{s \in B} [d_s]$ and the multi-index space we sum over is $\mathcal{J} = \prod_{s \in F} [d_s]$.
    In our example, the bound symbols are $B = \smallset{ij}$ and the free symbols are $F = \smallset{k}$.
    The multi-index space we iterate over is $d_i \times d_j = [3] \times [4]$.
    The multi-index space we sum over is $d_k = [5]$.

    From the definition of $\mathcal{I}$, it follows that $d_s$ has to be defined for all symbols $s \in B$.
    This means we have to add the constraint $\sigma(\bm{s_t}) \subseteq \bigcup_{i \in [n]} \sigma(\bm{s_i})$.

    However, we do not use every symbol in the multi-index spaces to index every input tensor.
    Instead, we use the index strings $\bm(s)$ to index the tensor.
    To formally express this, we need a projection from a multi-index $IJ \in \mathcal{I} \times \mathcal{J}$ to another multi-index, which includes only the symbols used in $\bm{s}$,
    in the same order as present in $\bm{s}$.
    We denote this as $IJ:\bm{s}$.
    Notice how this still allows duplication of indices given in $IJ$.
    This is needed, as can be seen in our example for $B_{iik}$,
    where a multi-index, e.g. $(1,4,2) \in \mathcal{I} \times \mathcal{J}$, is projected on the index string $iik$,
    which results in the multi-index $(1,4,2):iik = (1,1,2)$.

    In our example, we used the standard sum and multiplication as operators for computing our result.
    But with einsum, we allow the more general use of any semiring $R = (M, \oplus, \odot)$.
    With this, we can finally write down what computation a general einsum expression
    $$T := (\bm{s_1},\dots,\bm{s_n} \rightarrow \bm{s_t}, T^{(1)},\dots,T^{(n)})_R$$
    results in. It means that $T$ is a $\abs{\bm{s_t}}$-th order tensor with
    $$\forall I \in \mathcal{I}: T_{I: \bm{s_t}} = \bigoplus\limits_{J \in \mathcal{J}} \bigodot\limits_{i = 1}^{n} T^{(i)}_{IJ:\bm{s_t}}$$

    Because we also project the indices $I$ on the output string $\bm{s_t}$, we allow to iterate over duplicate indices,
    e.g. $\text{diag}(v) = (j \rightarrow jj, v)$.
    This leaves some entries of the result undefined.
    We define these entries to be the additive neutral element $\mymathbb{0}$ in the given semiring $R$.
    This may sound arbitrary at first, but will be useful for later theorems.

    There are still some special case which need to be considered.
    If there are no free symbols in the expression, then the sum will be empty.
    But we still want the result of the computation of the product.
    Therefore, if $F = \emptyset$, then
    $$T := (\bm{s_1},\dots,\bm{s_n} \rightarrow \bm{s_t}, T^{(1)},\dots,T^{(n)})_R$$
    results in the computation of a $\abs{\bm{s_t}}$-th order tensor $T$ with
    $$\forall I \in \mathcal{I}: T_{I: \bm{s_t}} = \bigodot\limits_{i = 1}^{n} T^{(i)}_{I:\bm{s_t}}$$
    If there are no bound symbols, we will sum over all axes given by the symbols in the expression.
    Therefore, if $B = \emptyset$, then
    $$T := (\bm{s_1},\dots,\bm{s_n} \rightarrow , T^{(1)},\dots,T^{(n)})_R$$
    results in the computation of a scalar $T$ with
    $$T = \bigoplus\limits_{J \in \mathcal{J}} \bigodot\limits_{i = 1}^{n} T^{(i)}_{J:\bm{s_t}}$$

    In case the semiring can be derived from the context, or if it is irrelevant, it can be left out from the expression.
\end{definition}

\subsection{Examples}
All following examples use the standard semiring $R = (\R, +, \cdot)$.
\begin{itemize}
    \item matrix-vector multiplication: Let $A \in \R^{m \times n}, v \in \R^{n}$. Then
          $$A \cdot v = (ij, j \rightarrow i, A, v)$$
    \item matrix-matrix multiplication: Let $A \in \R^{m \times r}, B \in \R^{r \times n}$. Then
          $$A \cdot B = (ik, kj \rightarrow ij, A, B)$$
    \item trace: Let $A \in \R^{n \times n}$. Then
          $$\text{trace}(A) = (ii \rightarrow, A)$$
    \item squared Frobenius norm: Let $A \in \R^{n \times n}$. Then
          $$\abs{A}_2^2 = (ij, ij \rightarrow,A,A)$$
    \item diagonal matrix: Let $v \in \R^{n}$. Then
          $$\text{diag}(v) = (i \rightarrow ii, v)$$
\end{itemize}

\subsection{Nested Einsum Expressions}
\begin{theorem}
    For $i \in [m + n]$, let $T^{(i)}$ be a $n_i$-th order tensor with index strings $\bm{s_i} \in S^{n_i}$.
    Let $\bm{s_u}, \bm{s_v}$ be index strings.
    Let
    $$U := (\bm{s_{m + 1}},\dots,\bm{s_{m + n}} \rightarrow \bm{s_u}, T^{(m + 1)},\dots,T^{(m + n)})$$
    and
    $$V := (\bm{s_1},\dots,\bm{s_m}, \bm{s_u} \rightarrow \bm{s_v}, T^{(1)},\dots,T^{(m)}, U)$$
    where the free symbols of the second einsum expression share no symbols with the first einsum expression.
    Then
    $$V = (\bm{s_1}, \dots, \bm{s_{m + n}} \rightarrow \bm{s_v}, T^{(1)}, \dots, T^{(m + n)})$$
\end{theorem}
\begin{proof}
    Let $B, B', F, F'$ be the bound and free symbols of the second and first einsum expression respectively.
    W.l.o.g. they are all non-empty.
    From them we can derive $\mathcal{I}, \mathcal{I}', \mathcal{J}, \mathcal{J}'$ as above.
    Then
    \begin{align*}
        V                                               & = (\bm{s_1},\dots,\bm{s_m}, \bm{s_u} \rightarrow \bm{s_v}, T^{(1)},\dots,T^{(m)}, U)                                                                                                                   \\
        \iff \forall I \in \mathcal{I}: V_{I: \bm{s_v}} & = \bigoplus\limits_{J \in \mathcal{J}} \bigodot\limits_{i = 1}^{m} T^{(i)}_{IJ:\bm{s_i}} \odot U_{IJ:\bm{s_u}}                                                                                         \\
                                                        & = \bigoplus\limits_{J \in \mathcal{J}} \bigodot\limits_{i = 1}^{m} T^{(i)}_{IJ:\bm{s_i}} \odot \bigoplus\limits_{J' \in \mathcal{J}'} \bigodot\limits_{i' = m + 1}^{m + n} T^{(i')}_{IJJ':\bm{s_{i'}}} \\
                                                        & = \bigoplus\limits_{J \in \mathcal{J}} \bigoplus\limits_{J' \in \mathcal{J}'} \bigodot\limits_{i = 1}^{m} T^{(i)}_{IJ:\bm{s_i}} \odot \bigodot\limits_{i = m + 1}^{m + n} T^{(i)}_{IJJ':\bm{s_{i'}}}   \\
                                                        & = \bigoplus\limits_{J \in \mathcal{J} \times \mathcal{J}'} \bigodot\limits_{i = 1}^{m + n} T^{(i)}_{IJ:\bm{s_i}}                                                                                       \\
        \iff V                                          & = (\bm{s_1}, \dots, \bm{s_{m + n}} \rightarrow \bm{s_v}, T^{(1)}, \dots, T^{(m + n)})
    \end{align*}
    where the third equality follows from
    $$\forall I' \in \mathcal{I}': U_{I': \bm{s_u}} = \bigoplus\limits_{J' \in \mathcal{J}'} \bigodot\limits_{i' = m + 1}^{m + n} T^{(i')}_{I'J':\bm{s_{i'}}},$$
    $B' \subseteq B \cup F$, and $(B \cup F) \cap F' = \emptyset$. The last two facts are required so that $IJJ':\bm{s_{i'}}$ is well-defined and projects on the same indices as $I'J':\bm{s_{i'}}$.
    The fourth equality follows from the distributivity in a semiring.
\end{proof}

\subsection{A More General Result}

\begin{theorem}
    For $i \in [m + n + 1]$, let $T^{(i)}$ be a $d^{(i)}$-th order tensor with index strings $s^{(i)} \in S^{d^{(i)}}$, $o := m + n + 1$.
    Also let $\hat{s}^{(o)}$ be alternative index strings for $T^{(o)}$ with $s^{(o)}_j = s^{(o)}_{j'} \implies \hat{s}^{(o)}_j = \hat{s}^{(o)}_{j'}$ for all $j, j' \in [d^{(o)}]$.
    Let
    $$T^{(0)} := (s^{(1)},\dots,s^{(m)}, \hat{s}^{(o)} \rightarrow s^{(0)}, T^{(1)},\dots,T^{(m)}, T^{(o)})$$
    and
    $$T^{(o)} = (s^{(m + 1)},\dots,s^{(m + n)} \rightarrow s^{(o)}, T^{(m + 1)},\dots,T^{(m + n)})$$
    where the free symbols of the second einsum expression share no symbols with the first einsum expression.
    Let $\nu: S \rightarrow S$ such that
    $$\nu(s) = \begin{cases}
            \hat{s}^{(o)}_j & \text{if }\exists j \in [d^{(o)}]: s^{(o)}_j = s \\
            s               & \text{else}
        \end{cases}$$
    which maps symbols in $s^{(o)}$ to the symbol at the same index in $\hat{s}^{(o)}$ and all other symbols to themselves.
    $\nu$ can be extended to map from axis symbol tuples by setting $\nu(s^{(i)}) \in S^{d^{(i)}}, \nu(s^{(i)})_j := \nu(s^{(i)}_j)$.

    Let $\hat{s}^{(i)} := \nu(s^{(i)})$
    Then
    $$T^{(0)} = (s^{(1)},\dots,s^{(m)}, \hat{s}^{(m + 1)}, \dots, \hat{s}^{(m + n)} \rightarrow s^{(0)}, T^{(1)},\dots,T^{(m + n)})$$
\end{theorem}
\begin{proof}
    % TODO: comment that we first want to show that we iterate over a subset of the multi-indices.
    Let $\mathcal{I}' = \prod_{s \in \sigma(\hat{s}_u)} [d_s]$.
    Let $\mathcal{M}:\bm{s} := \smallset{M: \bm{s} \mid M \in \mathcal{M}}$ for an index string $s$ and a multi-index space $\mathcal{M}$.
    Then $\mathcal{I}':\bm{\hat{s}_u} \subseteq \mathcal{I}:\bm{s_u}$, because $d_{s_{uj}} = d_{\hat{s}_{uj}}$ per the definition of einsum,
    and because the amount of axes contributing to $\mathcal{I}'$ ($\abs{\sigma(\bm{\hat{s}_u})}$) has to be smaller or equal to the amount of axes contributing to $\mathcal{I}$ ($\abs{\sigma(\bm{s_u})}$).
    This last fact follows from the constraint $s_{uj} = s_{uj'} \implies \hat{s}_{uj} = \hat{s}_{uj'}$.

    Then
    $$\forall I \in \mathcal{I}: U_{I: \bm{s_u}} = \bigodot\limits_{i = m + 1}^{m + n} T^{(i)}_{IJ:\bm{s_{i}}}$$
    and therefore
    $$\forall I \in \mathcal{I}': U_{I': \bm{\hat{s}_u}} = \bigodot\limits_{i = m + 1}^{m + n} T^{(i)}_{I'J:\bm{\hat{s}_{i}}}$$
    because of the previous observation,
    and because the free symbols of the expression, which are used in $J$, are not changed by the symbol map $\nu$.

    % TODO: consistent multi-index space names (I / I' get mixed up here)
    Therefore
    \begin{align*}
        V                                               & = (\bm{s_1},\dots,\bm{s_m}, \bm{\hat{s}_u} \rightarrow \bm{s_v}, T^{(1)},\dots,T^{(m)}, U)                                                                                                                   \\
        \iff \forall I \in \mathcal{I}: V_{I: \bm{s_v}} & = \bigoplus\limits_{J \in \mathcal{J}} \bigodot\limits_{i = 1}^{m} T^{(i)}_{IJ:\bm{s_i}} \odot U_{IJ:\bm{\hat{s}_u}}                                                                                         \\
                                                        & = \bigoplus\limits_{J \in \mathcal{J}} \bigodot\limits_{i = 1}^{m} T^{(i)}_{IJ:\bm{s_i}} \odot \bigoplus\limits_{J' \in \mathcal{J}'} \bigodot\limits_{i' = m + 1}^{m + n} T^{(i')}_{IJJ':\bm{\hat{s}_{i'}}} \\
                                                        & = \bigoplus\limits_{J \in \mathcal{J}} \bigoplus\limits_{J' \in \mathcal{J}'} \bigodot\limits_{i = 1}^{m} T^{(i)}_{IJ:\bm{s_i}} \odot \bigodot\limits_{i = m + 1}^{m + n} T^{(i)}_{IJJ':\bm{\hat{s}_{i'}}}   \\
                                                        & = \bigoplus\limits_{J \in \mathcal{J} \times \mathcal{J}'} \bigodot\limits_{i = 1}^{m} T^{(i)}_{IJ:\bm{s_i}} \odot \bigodot\limits_{i = m + 1}^{m + n} T^{(i)}_{IJ:\bm{\hat{s}_i}}                                                                                       \\
        \iff V                                          & = (\bm{s_1}, \dots, \bm{s_m}, \bm{\hat{s}_{m + 1}}, \dots, \bm{\hat{s}_{m + n}} \rightarrow \bm{s_v}, T^{(1)}, \dots, T^{(m + n)})
    \end{align*}
    where \dots
\end{proof}

\subsection{More Examples}
With these theorems, we can write some more complex expressions as einsum.
\begin{itemize}
    \item squared norm of matrix-vector multiplication: Let $A \in \R^{m \times n}, v \in \R^{n}$. Then
          \begin{align*}
              \abs{A \cdot v}_2^2 & = (i,i\rightarrow,(ij, j \rightarrow i, A, v),(ij, j \rightarrow i, A, v)) \\
                                  & = (ij,j,ij,j\rightarrow,A,v,A,v)
          \end{align*}
    \item trace of matrix-matrix multiplication: Let $A \in \R^{m \times n}, B \in \R^{n \times m}$. Then
          \begin{align*}
              \text{trace}(A \cdot B) & = (ii \rightarrow, (ik, kj \rightarrow ij, A, B)) \\
                                      & = (ik, ki \rightarrow, A, B)
          \end{align*}
    \item The theorem for this still has to be shown \dots:
          matrix multiplication with a diagonal matrix: Let $A \in \R^{m \times n}, v \in \R^{n}$. Then
          \begin{align*}
              A \cdot \text{diag}(v) & = (ik, kj \rightarrow ij, A, (i \rightarrow ii, v)) \\
                                     & = (ij, j \rightarrow ij, A, v)                      \\
          \end{align*}
\end{itemize}
