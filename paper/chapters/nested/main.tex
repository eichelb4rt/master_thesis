\chapter{Nested Expressions}
\label{chap:nested}

In practice, concatenations of operations come naturally, e.g. computing the squared norm of a matrix-vector product $\abs{A \cdot v}_2^2$
for $A \in \R^{m \times n}, v \in \R^n$.
This would lead to a nested Einsum expression $\abs{A \cdot v}_2^2 = (i,i \rightarrow , (ij, j \rightarrow i, A, v), (ij, j \rightarrow i, A, v))$.
This expression dictates the order of evaluating the expression.
In the example of the norm, the expression $(ij, j \rightarrow i, A, v)$ has to be evaluated before squaring and summing over the results of this computation.

This is limiting, because the order of evaluation might not yield optimal runtime.
This can be seen with a simple matrix-matrix-vector multiplication, which can be written as follows:
$$(A \cdot B) \cdot v = (ij, j \rightarrow i, (ik, kj \rightarrow ij, A, B), v)$$
which is clearly worse than the optimal contraction order
$$A \cdot (B \cdot v) = (ij, j \rightarrow i, A, (ij, j \rightarrow i, B, v))$$
for $A \in \R^{m \times r}, B \in \R^{r \times n}, v \in \R^n$.
Another limitation of nested Einsum strings is that we can not fully benefit from the computational advantages of a single Einsum string, that were stated in \cref{sec:einsum:comp_benefits}.

But fortunately, all nested Einsum expressions can be compressed into a single Einsum expression, if they are computed over the same semiring.
For instance,
\begin{align*}
    \abs{A \cdot v}_2^2 & = (i,i \rightarrow , (ij, j \rightarrow i, A, v), (ij, j \rightarrow i, A, v)) \\
                        & = (ij, j, ij, j \rightarrow, A, v, A, v)
\end{align*}
and
\begin{align*}
    (A \cdot B) \cdot v & = (ij, j \rightarrow i, (ik, kj \rightarrow ij, A, B), v) \\
                        & = (ik, kj, j \rightarrow i, A, B, v).
\end{align*}
This leaves the path of contraction up to the implementation, and lets us benefit from all the computational advantages mentioned in \cref{chap:einsum}.
In the following theorems, we assume that the computations are all over the same semiring $R = (M, \oplus, \odot)$.

\input{chapters/nested/simple.tex}

This means that we can compress all nested Einsum expressions, where the output string of the inner expression, which is used to compute $U$, is exactly the same as the respective input string in the outer expression, where $U$ is used as an input tensor.
This is already helpful for some naturally occuring expressions in linear algebra, e.g.
\begin{align*}
    \abs{A \cdot v}_2^2 & = (i,i\rightarrow,(ij, j \rightarrow i, A, v),(ij, j \rightarrow i, A, v)) \\
                        & = (ij,j,ij,j\rightarrow,A,v,A,v)
\end{align*}
for $A \in \R^{m \times n}, v \in \R^{n}$, or
\begin{align*}
    A \cdot B \cdot v & = (ij, j \rightarrow i, (ik, kj \rightarrow ij, A, B), v) \\
                      & = (ik,kj,j \rightarrow i, A, B, v)
\end{align*}
for $A \in \R^{m \times r}, B \in \R^{r \times n}, v \in \R^n$.
However, sometimes we need to access a different multi-index set than the one we computed, e.g.
$$\text{trace}(A \cdot B) = (ii \rightarrow, (ik, kj \rightarrow ij, A, B))$$
or
$$A \cdot \text{diag}(v) = (ik, kj \rightarrow ij, A, (i \rightarrow ii, v))$$
for $A \in \R^{m \times n}, B \in \R^{n \times m}, v \in \R^{n}$.
For this, we need more general ways of compressing nested Einsum expressions.

\input{chapters/nested/general.tex}

From this general theorem we can derive two more specific theorems,
which make the process of compressing simpler for a subset of nested expressions.

\input{chapters/nested/duplications.tex}
