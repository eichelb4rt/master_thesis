\chapter{Nested Expressions}
\label{chap:nested}

In practice, concatenations of operations arise naturally, e.g. computing the squared norm of a matrix-vector product $\abs{A \cdot v}_2^2$
for $A \in \R^{m \times n}, v \in \R^n$.
This would lead to a nested Einsum expression $\abs{A \cdot v}_2^2 = (i,i \rightarrow , (ij, j \rightarrow i, A, v), (ij, j \rightarrow i, A, v))$.
This expression dictates the order of evaluating the expression.
In the example of the norm, the expression $(ij, j \rightarrow i, A, v)$ has to be evaluated before squaring and summing over the results of this computation.

This is limiting, because the order of evaluation might not yield optimal runtime.
This can be seen with a simple matrix-matrix-vector multiplication, which can be written as follows:
$$(A \cdot B) \cdot v = (ij, j \rightarrow i, (ik, kj \rightarrow ij, A, B), v)$$
which is clearly worse than the optimal contraction order
$$A \cdot (B \cdot v) = (ij, j \rightarrow i, A, (ij, j \rightarrow i, B, v))$$
for $A \in \R^{m \times r}, B \in \R^{r \times n}, v \in \R^n$.
Another limitation of nested Einsum expressions is that we can not fully benefit from the computational advantages that come with the optimization of single Einsum expressions.

But fortunately, all nested Einsum expressions can be compressed into a single Einsum expression, if they are computed over the same semiring.
For instance,
\begin{align*}
    \abs{A \cdot v}_2^2 & = (i,i \rightarrow , (ij, j \rightarrow i, A, v), (ij, j \rightarrow i, A, v)) \\
                        & = (ij, j, ij, j \rightarrow, A, v, A, v)
\end{align*}
and
\begin{align*}
    (A \cdot B) \cdot v & = (ij, j \rightarrow i, (ik, kj \rightarrow ij, A, B), v) \\
                        & = (ik, kj, j \rightarrow i, A, B, v).
\end{align*}
This leaves the path of contraction up to the implementation, and lets us benefit from all the computational advantages mentioned in \cref{chap:einsum}.
For the following theorems, we assume that the computations are all over the same semiring $R = (M, \oplus, \odot)$.

\input{chapters/nested/simple.tex}

This means that we can compress all nested Einsum expressions, where the output string of the inner expression, which is used to compute $U$, is exactly the same as the respective input string in the outer expression, where $U$ is used as an input tensor.
This is already helpful for some naturally occuring expressions in linear algebra, e.g.
\begin{align*}
    \abs{A \cdot v}_2^2 & = (i,i\rightarrow,(ij, j \rightarrow i, A, v),(ij, j \rightarrow i, A, v)) \\
                        & = (ij,j,ij,j\rightarrow,A,v,A,v)
\end{align*}
for $A \in \R^{m \times n}, v \in \R^{n}$, or
\begin{align*}
    A \cdot B \cdot v & = (ij, j \rightarrow i, (ik, kj \rightarrow ij, A, B), v) \\
                      & = (ik,kj,j \rightarrow i, A, B, v)
\end{align*}
for $A \in \R^{m \times r}, B \in \R^{r \times n}, v \in \R^n$.
However, sometimes we need to access a different multi-index set than the one we computed, e.g.
$$\text{trace}(A \cdot B) = (ii \rightarrow, (ik, kj \rightarrow ij, A, B))$$
or
$$A \cdot \text{diag}(v) = (ik, kj \rightarrow ij, A, (i \rightarrow ii, v))$$
for $A \in \R^{m \times n}, B \in \R^{n \times m}, v \in \R^{n}$.
For this, we need more general ways of compressing nested Einsum expressions.

\input{chapters/nested/general.tex}

From this general theorem we can derive two more specific theorems,
which make the process of compressing simpler for a subset of nested expressions.

\input{chapters/nested/duplications.tex}

% TODO: can i really write "every"?
With these two more specific theorems, we can write every naturally occuring complex expression from linear algebra as a single Einsum expression.
The reason for this is, that in linear algebra, only up to two indices are used for a single tensor,
which means that with two index strings, it cannot happen that a duplication is removed and anonther duplication is introduced simultaniously.
Here are some more complex examples of expressions, which we can compress with the two more specific theorems:
\begin{itemize}
    \item Squared norm of matrix-vector multiplication: Let $A \in \R^{m \times n}, v \in \R^{n}$. Then
          \begin{align*}
              \abs{A \cdot v}_2^2 & = (i,i\rightarrow,(ij, j \rightarrow i, A, v),(ij, j \rightarrow i, A, v)) \\
                                  & = (ij,j,ij,j\rightarrow,A,v,A,v).
          \end{align*}
    \item Trace of matrix-matrix multiplication: Let $A \in \R^{m \times n}, B \in \R^{n \times m}$. Then
          \begin{align*}
              \text{trace}(A \cdot B) & = (ii \rightarrow, (ik, kj \rightarrow ij, A, B)) \\
                                      & = (ik, ki \rightarrow, A, B).
          \end{align*}
    \item Matrix multiplication with a diagonal matrix: Let $A \in \R^{m \times n}, v \in \R^{n}$. Then
          \begin{align*}
              A \cdot \text{diag}(v) & = (ik, kj \rightarrow ij, A, (i \rightarrow ii, v)) \\
                                     & = (ij, j \rightarrow ij, A, v).
          \end{align*}
\end{itemize}

But writing every expression from linear algebra as a single Einsum expression respectively was already possible before \cite{Klaus2023}.
With these theorems, we just derived a different way of achieving that.
For this, we can state a simple procedure.
First, every function is translated to their respective Einsum expression, which results in a nested Einsum expression.
Then, the nested expressions are compressed from the bottom up.

But there are naturally occuring examples of nested Einsum expressions, which need more than the two duplication theorems to be compressed.
For instance, when symbolically deriving Einsum expressions to find out if they compute a convex function,
nested statements can arise, that break duplications in both directions.
An example of a function, where this happens in the second derivative, is the trace of a matrix where all entries have been squared:
$$(ii,ii \rightarrow, A, A)$$
for some $A \in \R^{n \times n}$.
Now, with \cref{thm:nested_einsum:general}, this procedure allows us to compress even these expressions.
Generally, it allows us to write every computation, where the single steps can be translated to Einsum over a single semiring, as one big Einsum expression that does not blow up in size with levels of nesting.