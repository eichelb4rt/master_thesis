\section{Discussion}

The techniques that we were able to describe with a single Einsum expression respectively are convolution and max-pooling.
But even here, the mapping is not optimal because we still need design tensors to compute the permutations required for this techniques.
Because these design tensors have a potentially high order, they can be computationally inefficient.
We also showed that they can not be split up into a product of lower order tensors, which might have been a way to reduce the computational work load with the optimization of contraction paths.
However, this problem might be smaller in practice.
In the case of convolution in inference, the same constant masks are repeatedly used on different input data.
This allows us to precompute the contraction of the mask and reuse the result on different inputs.

We did not manage to describe the other presented examples with a single Einsum expression respectively.
Instead, we had to use a mixture of multiple Einsum expressions over different semirings and element-wise functions.
This means that Einsum is probably not powerful enough to describe the standard architectures in deep learning.
We do not know this for sure, as there might be semirings we did not explore, that allow to translate even these architectures naturally,
but we think it is unlikely, because of the high number of operations used both in the combination of tensor entries and in the aggregation over axes.
Another possibility which we did not explore is, that there are other models which approximate standard architectures, and therefore produce similar outputs, that can naturally be translated to Einsum with the semirings we used.
Models like this could be explored in future work.
